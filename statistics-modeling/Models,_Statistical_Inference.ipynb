{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Parametric and NonParametric Models\n",
        "\n",
        "**Statistical model** is a set of distributions ${F}$.\n",
        "\n",
        "**Parametric Model** is a set ${F}$ that can be parameterized by a finite number of parameters.\n",
        "\n",
        "For example with data from a normal distribution,\n",
        "$$ {F} = \\left\\{ f(x; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2\\sigma^2} (x -\\mu)^2\\right), \\mu \\in \\mathbb{R}, \\sigma >0 \\right\\} $$\n",
        "\n",
        "In general, a **parametric model** takes the form where $\\theta$ is an unknown parameter that takes values in the parameter space $\\Theta$. $$ {F} = \\{f(x; \\theta): \\theta \\in \\Theta\\} $$\n",
        "\n",
        "If $\\theta$ is a vector and we're interested in one component of $\\theta$, the remaining parameters are named nuisance parameters.\n",
        "\n",
        "Nonparametric Model\n",
        "A nonparametric model is a set ${F}$ that cannot be parameterized by a finite number of parameters.\n",
        "\n",
        "Parametric Model Notation\n",
        "If ${F} = \\{f(x; \\theta): \\theta \\in \\Theta\\}$ is a parametric model, we note:\n",
        "\n",
        "**Probability:**\n",
        "$$ P_{\\theta}(X \\in A) = \\int_A f(x; \\theta) \\, dx $$\n",
        "**Expectation:**\n",
        "$$ E_{\\theta}(X) = \\int x \\cdot f(x; \\theta) \\, dx $$"
      ],
      "metadata": {
        "id": "uRWxNADKK4sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concepts in Inference\n",
        "\n",
        "Let $X_1, ..., X_n$ be $n$ iid data points from some distribution $F$.\n",
        "\n",
        "**Point Estimator**\n",
        "A point estimator $\\hat{\\theta}_n$ of a parameter $\\theta$ is some function\n",
        "$$ \\hat{\\theta}_n = g(X_1, ..., X_n) $$\n",
        "\n",
        "**Bias**\n",
        "We define the bias of $\\hat{\\theta}_n$ as:\n",
        "$$ \\text{bias}(\\hat{\\theta}_n) = E_{\\theta}(\\hat{\\theta}_n) - \\theta $$\n",
        "We say that $\\hat{\\theta}_n$ is unbiased if $E_{\\theta}(\\hat{\\theta}_n) = \\theta$.\n",
        "\n",
        "**Consistent Estimator**\n",
        "A point estimator $\\hat{\\theta}_n$ of a parameter $\\theta$ is consistent if $\\hat{\\theta}_n \\xrightarrow{P} \\theta$.\n",
        "\n",
        "**Sampling Distribution**\n",
        "The distribution of $\\hat{\\theta}_n$ is called the sampling distribution.\n",
        "\n",
        "**Standard Error**\n",
        "The standard deviation of $\\hat{\\theta}_n$ is called the standard error, denoted by se.\n",
        "$$ \\text{se} = \\text{se}(\\hat{\\theta}_n) = \\sqrt{V(\\hat{\\theta}_n)} $$\n",
        "\n",
        "Often it is not possible to compute the standard error, but usually we can estimate the standard error."
      ],
      "metadata": {
        "id": "pEs0R90hTr11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Square Error (MSE)** measures the quality of a point estimate.\n",
        "\n",
        "- $ \\text{MSE} = E_{\\theta}(\\hat{\\theta}_n - \\theta)^2 $,    \n",
        "- $ \\text{MSE} = \\text{bias}(\\hat{\\theta}_n)^2 + V_{\\theta}(\\hat{\\theta}_n) $\n"
      ],
      "metadata": {
        "id": "FkON5Sc7YUI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Sets\n",
        "\n",
        "**Confidence Interval**\n",
        "\n",
        "A $(1 - \\alpha)$ confidence interval for a parameter $\\theta$ is an interval $C_n = (a, b)$, where\n",
        "\n",
        "$a = a(X_1, \\ldots, X_n)$ and $b = b(X_1, \\ldots, X_n)$ are functions of the data such that:\n",
        "$ P(\\theta \\in C_n) \\geq 1 - \\alpha, \\quad \\forall \\theta \\in \\Theta $\n",
        "\n",
        "The value $(1 - \\alpha)$ is the coverage of the confidence interval.\n",
        "\n",
        "$C_n$ is random, and $\\theta$ is fixed.\n",
        "\n",
        "Confidence Set\n",
        "If $\\theta$ is a vector, then we use a confidence set instead of an interval.\n",
        "\n",
        "\n",
        "**Theorem: Normal-based Confidence Interval**\n",
        "\n",
        "Suppose that $\\hat{\\theta}_n \\sim N(\\theta, \\hat{se}^2)$.\n",
        "Let $\\Phi$ be the CDF of a standard normal distribution, and\n",
        "\n",
        "let $z_{\\alpha/2} = \\Phi^{-1}(1 - (\\alpha/2))$.\n",
        "\n",
        "Then, $ P(Z > z_{\\alpha/2}) = \\frac{\\alpha}{2} \\quad \\text{and} \\quad P(-z_{\\alpha/2} < Z < z_{\\alpha/2}) = 1 - \\alpha, \\quad $\n",
        "\n",
        "where $Z \\sim N(0, 1) $\n",
        "\n",
        "Let $C_n = (\\hat{\\theta}_n - z_{\\alpha/2} \\cdot \\hat{se}, \\hat{\\theta}_n + z_{\\alpha/2} \\cdot \\hat{se})$.\n",
        "\n",
        "Then, $P_{\\theta}(\\theta \\in C_n) \\rightarrow 1 - \\alpha$.\n"
      ],
      "metadata": {
        "id": "R1ZZVS2aW7z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Testing"
      ],
      "metadata": {
        "id": "RvEzGCCpai6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametric Inference"
      ],
      "metadata": {
        "id": "S5qWA33wbqrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametric Models and Inference\n",
        "\n",
        "**Parametric models** are noted in the form $F = \\{f(x; \\theta): \\theta \\in \\Theta\\}$.\n",
        "\n",
        "Here, $\\Theta \\in \\mathbb{R}^k$ is the parameter space, and $\\theta = (\\theta_1, \\ldots, \\theta_k)$ is the parameter.\n",
        "\n",
        "**The Problem of Inference** reduces to the problem of estimating the parameter $\\theta$.\n",
        "\n",
        "Parameter of Interest and Nuisance Parameter\n",
        "We are often interested in a function $T(\\theta)$.\n",
        "\n",
        "For example, if $X \\sim N(\\mu, \\sigma^2)$, the parameter is $\\theta = (\\mu, \\sigma)$.\n",
        "\n",
        "If the goal is to estimate $\\mu = T(\\theta)$, then $\\mu$ is called the parameter of interest, and $\\sigma$ is called the nuisance parameter."
      ],
      "metadata": {
        "id": "1FtZlmQnhy6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method of Moments Estimator\n",
        "\n",
        "Suppose that the parameter $\\theta = (\\theta_1, \\dots, \\theta_k)$ has $k$ components.\n",
        "For $1 \\leq j \\leq k$, define the $j$-th moment\n",
        "$$ \\alpha_j \\equiv \\alpha_j(\\theta) \\equiv E_{\\theta}(X^j) = \\int x^j \\, dF_{\\theta}(x) $$\n",
        "\n",
        "and the $j$-th sample moment\n",
        "$$ \\hat{\\alpha}_j = \\frac{1}{n}\\sum_{i=1}^{n} X_i^j \\quad \\text{for } i \\in [1, n] $$\n",
        "\n",
        "**The method of moments estimator** $\\hat{\\theta}_n$ is defined to be the value of $\\theta$\n",
        "such that\n",
        "\\begin{align*}\n",
        "\\alpha_1(\\hat{\\theta}_n) &= \\hat{\\alpha}_1 \\\\\n",
        "\\alpha_2(\\hat{\\theta}_n) &= \\hat{\\alpha}_2 \\\\\n",
        "& \\dots \\\\\n",
        "\\alpha_k(\\hat{\\theta}_n) &= \\hat{\\alpha}_k\n",
        "\\end{align*}\n",
        "This defines a system of $k$ equations with $k$ unknowns.\n",
        "\n",
        "**Theorem**\n",
        "Let $\\hat{\\theta}_n$ denote the method of moments estimator. Given some conditions:\n",
        "- The estimate $\\hat{\\theta}_n$ exists with probability tending to 1.\n",
        "- The estimate is consistent: $\\hat{\\theta}_n \\xrightarrow{P} \\theta$ (converges in Probability).\n",
        "- The estimate is asymptotically Normal:\n",
        "$ \\sqrt{n}(\\hat{\\theta}_n - \\theta) \\xrightarrow{d} N(0, \\Sigma) $\n",
        "where $\\Sigma = gE_{\\theta}(Y T^T) g^T$,\n",
        "$ Y = (X, X^2, ..., X^k)^T, \\quad g = (g_1, ..., g_k)^T $ and\n",
        "$ g_j = \\frac{\\delta(\\alpha_j^{-1}(\\theta))}{\\delta\\theta} $"
      ],
      "metadata": {
        "id": "AnNPaufDfxOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximum Likelihood\n",
        "\n",
        "Let X\\_1, â€¦, X\\_n be i.i.d. random variables with probability density function (pdf) f\\(x; \\\\theta\\)\n",
        "\n",
        "The **Likelihood function** is defined as:\n",
        "$$ L_n(\\theta) = \\prod_{i=1}^{n} f(X_i; \\theta) $$\n",
        "\n",
        "The **Log-likelihood function** is defined as:\n",
        "$$ l_n(\\theta) = \\log L_n(\\theta) = \\sum_{i=1}^{n} \\log f(X_i; \\theta) $$\n",
        "\n",
        "The likelihood function L_n is the joint pdf, but treated as a function of the parameter.\n",
        "\n",
        "L_n, and the likelihood function is not a pdf, so not true it integrates to 1.\n",
        "\n",
        "**Maximum Likelihood Estimator (MLE)**\n",
        "The maximum likelihood estimator (MLE), is the value that maximizes L_n."
      ],
      "metadata": {
        "id": "gZA30L5rBO_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Properties of MLE\n",
        "\n",
        "Given certain conditions on the model, the MLE \\\\hat\\{\\\\theta\\}\\_n is a desirable estimator due to its properties:\n",
        "\n",
        "**Consistency**:\n",
        "\n",
        "The MLE is consistent, meaning it converges in probability to the true value of the parameter \\\\theta:\n",
        "$$ \\hat{\\theta}_n \\xrightarrow{p} \\theta $$\n",
        "where \\\\theta is the true value of the parameter.\n",
        "\n",
        "**Kullback-Leibler Distance and Moment**:\n",
        "\n",
        "The Kullback-Leibler distance between two pdfs f and g is given by:\n",
        "$$ D(f, g) = \\int f(x) \\log\\left(\\frac{f(x)}{g(x)}\\right) dx $$\n",
        "The moment of \\\\theta, M\\_n\\(\\\\theta\\), is related to the Kullback-Leibler distance:\n",
        "$$ M_n(\\theta) \\sim -D(\\theta_n, \\theta) $$\n",
        "\n",
        "**Equivalence**:\n",
        "\n",
        "If $\\hat{\\theta}_n$ is the MLE of $\\theta$, and $\\tau = g(\\theta)$ is a one-to-one transformation of $\\theta$, then the MLE of $\\tau$ is $\\hat{\\tau}_n = g(\\hat{\\theta}_n)$.\n",
        "\n",
        "**Asymptotic Normality**:\n",
        "\n",
        "The MLE is asymptotically normal:\n",
        "$$ \\sqrt{n}(\\hat{\\theta}_n - \\theta) / \\hat{se} \\xrightarrow{d} N(0, 1) $$\n",
        "\n",
        "where $\\hat{se}$ is the estimated standard error.\n",
        "\n",
        "Score Function and Fisher Information: The score function is defined as:\n",
        "$$ s(X; \\theta) = \\frac{d}{d\\theta} \\log(f(X; \\theta)) $$\n",
        "    \n",
        "The Fisher information for a sample of size $n$ is:\n",
        "$$ I_n(\\theta) = V_{\\theta}\\left(\\sum_{i=1}^{n} s(X_i; \\theta)\\right) $$\n",
        "\n",
        "**Asymptotic Optimality (Efficiency)**:\n",
        "Among well-behaved estimators, the MLE is asymptotically optimal or efficient, meaning it has the smallest variance for large samples (at least). Under certain regularity conditions, let $se = \\sqrt{1/I_n(\\theta)}$. Then:\n",
        "$$ (\\hat{\\theta}_n - \\theta) / se \\xrightarrow{d} N(0, 1) $$\n",
        "\n",
        "Similarly, using the estimated Fisher information $I_n(\\hat{\\theta}_n)$, let $\\hat{se} = \\sqrt{1/I_n(\\hat{\\theta}_n)}$. Then:\n",
        "$$ (\\hat{\\theta}_n - \\theta) / \\hat{se} \\xrightarrow{d} N(0, 1) $$\n",
        "\n",
        "**Approximation to Bayes Estimator**:\n",
        "\n",
        "MLE is approximately the Bayes estimator under certain conditions.\n"
      ],
      "metadata": {
        "id": "Llq0DKcUHD_n"
      }
    }
  ]
}