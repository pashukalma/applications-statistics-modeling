{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis (PCA)\n",
        "Consider a sample $X_1, \\dots, X_n$ which forms a set of points in $\\mathbb{R}^d$.\n",
        "\n",
        "Is it possible to project this set onto a linear subspace of dimension $d' < d$ while retaining as much information as possible?\n",
        "\n",
        "PCA achieves this by preserving as much covariance structure as possible, by identifying orthogonal directions that best discriminate the points of the set."
      ],
      "metadata": {
        "id": "igygyolT2bCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Idea, Write $S = PDP^T$, where:\n",
        "\n",
        "- $P = (v_1, \\dots, v_d)$ is an orthogonal matrix, i.e., $\\|v_j\\|_2 = 1$ and $v_j^T v_k = 0$ for $j \\neq k$.\n",
        "- $D = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)$, with $\\lambda_1 \\geq \\dots \\geq \\lambda_d \\geq 0$.\n",
        "\n",
        "- Note that $D$ is the empirical covariance matrix of $P^T X_i$'s, for $i = 1, \\dots, n$.\n",
        "- In particular, $\\lambda_1$ is the empirical variance of $v_1^T X_i$, and $\\lambda_d$ is the empirical variance of $v_d^T X_i$.\n",
        "- Each $\\lambda_j$ measures the spread of the set in the direction $v_j$.\n",
        "- In particular, $v_1$ is the direction of maximal spread.\n",
        "- Indeed, $v_1$ maximizes the empirical covariance of $a^T X_1, \\dots, a^T X_n$, over $a \\in \\mathbb{R}^d$ such that $\\|a\\|_2 = 1$.\n",
        "- Proof: For any unit vector $a$, show that $a^T \\Sigma a = (P^T a)^T D (P^T a) \\leq \\lambda_1$, with equality if $a = v_1$.\n",
        "\n",
        "\n",
        "- Proof: For any unit vector  $a$ in $\\mathbb{R}^d$,\n",
        "$ a^T \\Sigma a = a^T (PDP^T) a = (P^T a)^T D (P^T a) $\n",
        "\n",
        "  with  equality holds when $a = v_1$."
      ],
      "metadata": {
        "id": "D8gRpldyA0Jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis Main Principle\n",
        "Idea of the PCA: find the collection of orthogonal directions in which the set is spread out.\n",
        "\n",
        "- $v_1 \\in \\underset{u}{\\operatorname{argmax}} u^T \\Sigma u \\text{subject to } u|| = 1 $\n",
        "\n",
        "  $v_2 \\in \\underset{u}{\\operatorname{argmax}} u^T \\Sigma u \\text{subject to } u|| = 1, \\quad u \\perp v_1 $\n",
        "\n",
        "  $v_d \\in \\underset{u}{\\operatorname{argmax}} u^T \\Sigma u \\text{subject to }\n",
        "u|| = 1, u \\perp v_j, \\quad j = 1, \\dots, d-1$\n",
        "  where $\\Sigma$ covariance matrix.\n",
        "\n",
        "- The $k$ orthogonal directions in which the set is most spread out correspond to the eigenvectors associated with the $k$-largest eigenvalues of $\\Sigma$. They are called principal directions."
      ],
      "metadata": {
        "id": "ZRpXzz723-UY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis Algorithm\n",
        "- Input $X_1, \\dots, X_n$, a set of $n$ points in dimension $d$.\n",
        "- Compute the empirical covariance matrix $S$.\n",
        "- Compute the spectral decomposition $S = PDP^T$, where $D = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)$,\n",
        "\n",
        "  with $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_d$, and $P = (v_1, \\dots, v_d)$ is an orthogonal matrix.\n",
        "- Choose $k < d$ and set $P_k = (v_1, \\dots, v_k) \\in \\mathbb{R}^{d \\times k}$.\n",
        "- Output $Y_1, \\dots, Y_n$, where $Y_i = P_k^T X_i \\in \\mathbb{R}^k$, for $i = 1, \\dots, n$.\n"
      ],
      "metadata": {
        "id": "7j7wdv8aVrn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications\n",
        "PCA in statistics is used for estimation and Machine learning\n",
        "- In genomics applications for gene expression we use sparse PCA\n",
        "- It may be known beforehand that Sigma has low rank\n",
        "- Running PCA on Sigma, $S ~S'$,   $S'=P Sigma P^T$\n",
        "- S' is a better estimator of S under low-rank assumption\n"
      ],
      "metadata": {
        "id": "vSfwXIngRe-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________"
      ],
      "metadata": {
        "id": "Kjan7ABy4NKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Appendix**"
      ],
      "metadata": {
        "id": "5DrXO8VpytZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Algebra\n",
        "\n",
        "- $\\Sigma$ and $S$ are symmetric, positive semi-definite matrices.\n",
        "- Any real symmetric matrix $A \\in \\mathbb{R}^{d \\times d}$ has the spectral decomposition $A = PDP^T$\n",
        "    where:\n",
        "    - $P$ is a $d \\times d$ orthogonal matrix, i.e., $PP^T = P^TP = I_d$.\n",
        "    - $D$ is a diagonal matrix.\n",
        "\n",
        "\n",
        "- The diagonal elements of $D$ are the eigenvalues of $A$, and the columns of $P$ are the eigenvectors of $A$.\n",
        "- $A$ is positive semi-definite if and only if all its eigenvalues are non-negative."
      ],
      "metadata": {
        "id": "hGUMIc25rkBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate Statistics (1)\n",
        "- Let $X$ be a $d$-dimensional r.v. and $X_1, \\ldots, X_n$ be $n$ independent copies of $X$.\n",
        "\n",
        "$$X_i = (X_i^{(1)}, \\ldots, X_i^{(d)})^T, \\quad i = 1, \\ldots, n$$\n",
        "\n",
        "- Denote by $X$ the random $n \\times d$ matrix\n",
        "\n",
        "$$X = \\begin{bmatrix}\n",
        "X_1^T \\\\\n",
        "\\vdots \\\\\n",
        "X_n^T\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "- Assume $\\mathbb{E}[\\|X\\|_2^2] < \\infty$.\n",
        "\n",
        "- Mean of $X$: $\\mathbb{E}[X] = (\\mathbb{E}[X^{(1)}], \\ldots, \\mathbb{E}[X^{(d)}])^T$.\n",
        "\n",
        "- Covariance matrix of $X$, $\\Sigma = (\\sigma_{j,k})_{j,k}$, where $j, k = 1, \\ldots, d$, and $\\sigma_{j,k} = \\mathrm{cov}(X^{(j)}, X^{(k)})$.\n",
        "\n",
        "$$\\Sigma = \\mathbb{E}[XX^T] - \\mathbb{E}[X]\\mathbb{E}[X]^T = \\mathbb{E}[(X - \\mathbb{E}[X])(X - \\mathbb{E}[X])^T]$$\n",
        "\n",
        "- Empirical mean of $X_1, \\ldots, X_n$:\n",
        "\n",
        "$$\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i = (\\bar{X}_1, \\ldots, \\bar{X}_d)^T$$\n",
        "\n",
        "- Empirical covariance of $X_1, \\ldots, X_n$, the matrix $S = (s_{j,k})$, where $s_{j,k}$ is the empirical covariance of $X_i^{(j)}, X_i^{(k)}$, $i = 1, \\ldots, n$.\n",
        "\n",
        "$$S = \\frac{1}{n} \\sum_{i=1}^n (X_i X_i^T - \\bar{X}_n \\bar{X}_n^T) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}_n)(X_i - \\bar{X}_n)^T$$"
      ],
      "metadata": {
        "id": "cAmJysVZkF12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_______________________"
      ],
      "metadata": {
        "id": "IsiWOZbPqNsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multivariate Statistics (2)\n",
        "- Note that\n",
        "$\\bar{X} = \\frac{1}{n} X^T \\mathbf{1}_n, \\quad \\mathbf{1}_n = (1, \\dots, 1)^T \\in \\mathbb{R}^n.$\n",
        "\n",
        "- Note that $S = \\frac{1}{n} XX^T - \\frac{1}{n^2} X \\mathbf{1}_n \\mathbf{1}_n^T X = \\frac{1}{n} X^T H X, \\quad \\text{where} \\quad H = I_n - \\frac{1}{n} \\mathbf{1}_n \\mathbf{1}_n^T.$\n",
        "\n",
        "- $H$ is an orthogonal projector: $H^2 = H$, $H^T = H$.\n",
        "\n",
        "- If $u \\in \\mathbb{R}^d$,\n",
        "\n",
        "    - $u^T \\Sigma u = \\text{var}(u^T X).$\n",
        "\n",
        "    - $u^T S u$ is the sample variance of $u^T X_1, \\dots, u^T X_n$.\n",
        "\n",
        "- $u^T S u$ measures how spread diverse the points are in $u$.\n",
        "\n",
        "- If $u^T S u = 0$, then all $X_i$'s are in an affine subspace orthogonal to $u$.\n",
        "\n",
        "- If $u^T \\Sigma u$ is large with $\\|u\\|_2 = 1$, then the direction of $u$ explains the spread of the sample."
      ],
      "metadata": {
        "id": "Ch4xIwZuoYpx"
      }
    }
  ]
}